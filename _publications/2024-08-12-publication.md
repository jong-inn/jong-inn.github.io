---
title: "Benchmarking Cognitive Biases in Large Language Models as Evaluators"
date: 2024-08-12
image: /assets/figures/cobbler_pipeline.jpg
categories: research
authors: "Ryan Koo, Minhwa Lee, Vipul Raheja, <u><strong>Jong Inn Park</strong></u>, Zae Myung Kim, Dongyeop Kang"
venue: "Findings of ACL"
arxiv: https://arxiv.org/abs/2309.17012
page: https://minnesotanlp.github.io/cobbler-project-page/
code: https://github.com/minnesotanlp/cobbler
data: https://minnesotanlp.github.io/cobbler-project-page/demo/index.html
---
Evaluated 16 large language models (LLMs) as automatic evaluators using preference ranking and introduced the Cognitive Bias Benchmark for LLMs as Evaluators (COBBLER), revealing significant cognitive biases and misalignment with human preferences, indicating limitations in using LLMs for automatic annotation.